# Gradient Descent: Lesson Plan

There are three parts to the case study:

- numerical differentiation
- symbolic differentiation
- automatic differentiation

This can be motivated by :sparkle: artificial intelligence :sparkle:. Deep learning uses gradient descent, which relies on differentiation. It uses automatic differentiation in particular.

## Goals

The goals are:

- gain some familiarity manipulating functions (numerical differentiation)
- see a more advanced use of algebraic data types (symbolic differentiation)
- get closer to the frontier of current research and practice (automatic differentiation)
- understand how differentiation is a higher-order function


The online material should walk through the material. Don't forget there is a supporting code repository: https://github.com/creativescala/case-study-gradient-descent/

The main problems people have is:

1. grasping the idea of differentiation as a function that manipulates a function. Most people think of the derivative of a function at a point, and not the derivative itself that is a function.

2. keeping all the concepts separate in performing gradient descent.
